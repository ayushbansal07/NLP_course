{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Task 1\n",
    "### Generative vs Discriminative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generative** : Naive Bayes classifier, Gaussian Mixture model, GANs, LDA(Latent Dirichlet Allocation) <br>\n",
    "**Discriminative** : Neural networks, Logistic regression, SVM<br> <br>\n",
    "**Explainations** : <br>\n",
    "**Generative Models** : These models are considered generative because they estimate the conditional prob p(x|y) by actually calculating joint Probabilty p(x,y) and diving it by prior distribution p(x) i.e., p(x|y) = p(x,y)/p(x). Therefore, it is considered Genrative.<br>\n",
    "**Discriminative Models** : Each of these models are used in classification task where we have to find out the probability of of output y given input x and these networks directly learn the conditional probability distribution p(y|x) without the need of any prior distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Task 2\n",
    "### Hidden Markov Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.corpus import treebank,brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = brown.tagged_sents(tagset='universal')[:-100] \n",
    "test_data= brown.tagged_sents(tagset='universal')[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dict={}\n",
    "word_dict={}\n",
    "\n",
    "for sent in corpus:\n",
    "    for elem in sent:\n",
    "        if elem[0] not in word_dict:\n",
    "            word_dict[elem[0]] = 0\n",
    "        if elem[1] not in tag_dict:\n",
    "            tag_dict[elem[1]] = 0\n",
    "        word_dict[elem[0]] += 1\n",
    "        tag_dict[elem[1]] += 1\n",
    "print(\"Number of words in dict : \",len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_list = {}\n",
    "tags_rev_list = []\n",
    "i = 0\n",
    "for tag, ct in tag_dict.items():\n",
    "    tags_list[tag] = i\n",
    "    tags_rev_list.append(tag)\n",
    "    i += 1\n",
    "NUM_TAGS = len(tags_list)\n",
    "word_list = {}\n",
    "i = 0\n",
    "for word, ct in word_dict.items():\n",
    "    word_list[word] = i\n",
    "    i += 1\n",
    "NUM_WORDS = len(word_list)\n",
    "print(\"Tags List : \", tags_list)\n",
    "print(\"Reversed Tag List : \",tags_rev_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.zeros(NUM_TAGS)\n",
    "start_tags_ct = {}\n",
    "total = 0\n",
    "for sent in corpus:\n",
    "    start_tag = sent[0][1]\n",
    "    if start_tag not in start_tags_ct:\n",
    "        start_tags_ct[start_tag] = 0\n",
    "    start_tags_ct[start_tag] += 1\n",
    "    total += 1\n",
    "for start_tag, ct in start_tags_ct.items():\n",
    "    S[tags_list[start_tag]] = ct/total\n",
    "print(\"Start Matrix with dimensions\",S.shape,\":\")\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transition Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.zeros((NUM_TAGS,NUM_TAGS))\n",
    "transition = {}\n",
    "for sent in corpus:\n",
    "    ln = len(sent)\n",
    "    for i in range(1,ln):\n",
    "        prev = sent[i-1][1]\n",
    "        curr = sent[i][1]\n",
    "        if prev not in transition:\n",
    "            transition[prev] = {}\n",
    "        if curr not in transition[prev]:\n",
    "            transition[prev][curr] = 0\n",
    "        transition[prev][curr] += 1\n",
    "for prev,val in transition.items():\n",
    "    total = 0\n",
    "    for curr, ct in val.items():\n",
    "        total += ct\n",
    "    for curr, ct in val.items():\n",
    "        P[tags_list[prev]][tags_list[curr]] = ct/total\n",
    "print(\"Transition Matrix with dimensions\",P.shape,\":\")\n",
    "print(P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emission Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "O = np.zeros((NUM_TAGS,NUM_WORDS))\n",
    "emission = {}\n",
    "for sent in corpus:\n",
    "    for elem in sent:\n",
    "        word = elem[0]\n",
    "        tag = elem[1]\n",
    "        if tag not in emission:\n",
    "            emission[tag] = {}\n",
    "        if word not in emission[tag]:\n",
    "            emission[tag][word] = 0\n",
    "        emission[tag][word] += 1\n",
    "        \n",
    "for tag, val in emission.items():\n",
    "    total = 0\n",
    "    for word, ct in val.items():\n",
    "        total += ct\n",
    "    for word, ct in val.items():\n",
    "        O[tags_list[tag]][word_list[word]] = ct/total\n",
    "print(\"Emission Matrix with dimensions\",O.shape,\":\")\n",
    "print(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Formulation of Viterbi Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi(sequence):\n",
    "    M = len(sequence)\n",
    "    state_len = NUM_TAGS\n",
    "    \n",
    "    T1 = np.zeros((state_len,M))\n",
    "    T2 = np.zeros((state_len,M))\n",
    "    \n",
    "    for state in range(state_len):\n",
    "        prob = 1e-100\n",
    "        if sequence[0] in word_list:\n",
    "            prob = O[state,word_list[sequence[0]]]\n",
    "        T1[state,0] = S[state]*prob\n",
    "        T2[state,0] = -1\n",
    "    for i in range(1,M):\n",
    "        for state in range(state_len):\n",
    "            prob = 1e-10\n",
    "            if sequence[i] in word_list:\n",
    "                prob = O[state,word_list[sequence[i]]]\n",
    "            temp = T1[:,i-1]*P[:,state]*prob\n",
    "            T1[state,i] = np.max(temp)\n",
    "            T2[state,i] = np.argmax(temp)\n",
    " \n",
    "    seq = np.zeros(M)\n",
    "    seq[M-1] = np.argmax(T1[:,M-1])\n",
    "    best_score = np.max(T1[:,M-1])\n",
    "    for state in range(M-2,-1,-1):\n",
    "        seq[state] = T2[int(seq[state+1]),state+1]    \n",
    "    return seq, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_sents = []\n",
    "test_tags = []\n",
    "for sent in test_data:\n",
    "    test_sents.append([x[0] for x in sent])\n",
    "    test_tags.append([x[1] for x in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq,best_score = viterbi(test_sents[0])\n",
    "total_correct = 0\n",
    "total = 0\n",
    "test_labels = []\n",
    "for i,sent in enumerate(test_sents):\n",
    "    seq, best_score = viterbi(sent)\n",
    "    print(\"Sentence : \", ' '.join(sent))\n",
    "    print(\"Best Score : \",best_score)\n",
    "    res_tags = [tags_rev_list[int(x)] for x in seq]\n",
    "    test_labels.append(res_tags)\n",
    "    print(\"Best Sequence : \",res_tags)\n",
    "    print(\"Actual Sequence : \", test_tags[i])\n",
    "    correct = np.mean(np.array(res_tags)==np.array(test_tags[i]))\n",
    "    total_correct += np.sum(np.array(res_tags)==np.array(test_tags[i]))\n",
    "    total += len(res_tags)\n",
    "    print(\"Accuracy : \",correct*100,\"%\")\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "print(\"Overall Accuracy : \",total_correct,\"/\",total,\" = \",(total_correct*100/total),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"F1 score for HMM Model :\")\n",
    "print(metrics.flat_f1_score(test_tags, test_labels, \n",
    "                      average='weighted', labels=tags_rev_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tag wise report for HMM Model\")\n",
    "print(metrics.flat_classification_report(\n",
    "    test_tags, test_labels, labels=tags_rev_list, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Task 3\n",
    "### Conditional Random Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sents = corpus\n",
    "def word2features(sent,i):\n",
    "    word = sent[i][0]\n",
    "    \n",
    "    features ={\n",
    "    'bias': 1.0,\n",
    "    'length_of_word' : len(word),\n",
    "    'startsWithUpper' : word[0].isupper(),\n",
    "    'lower' : word.lower(),\n",
    "    'word_index' : i,\n",
    "    'length_of_sentence' : len(sent),\n",
    "    }\n",
    "                \n",
    "    return features\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent,i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for i,label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=[sent2features(s) for s in train_sents]\n",
    "y_train=[sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test=[sent2features(s) for s in test_data]\n",
    "y_test=[sent2labels(s) for s in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "labels=list(crf.classes_)\n",
    "print(\"F1 score for CRF model : \")\n",
    "print(metrics.flat_f1_score(y_test, y_pred, \n",
    "                      average='weighted', labels=labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels, \n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(\"Tag Wise report for CRF model :\")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CRF features justification\n",
    "**bias** - default<br>\n",
    "**length_of_word** - distribution of length of word for different POS tags can vary<br>\n",
    "**startsWithUpper** - some nouns have their first letter as uppercase. Also, it gave a good improvement on the f1 score<br>\n",
    "**lower** - we make all the words lowercase for most of the nlp tasks, so the intuition of providing the lowercase word as a feature is good so that same words with different cases have atleast this feature same. Showed a tremendous imporvement in f1 scores (from ~0.40 to ~0.94)<br>\n",
    "**word_index and length_of_sentence** - The relative position of the word in the sentence is a good feature for assigning POS tags. Eg -  The subjects or nouns usually come at the start and verb comes later. Improved the f1 score to 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
